\begin{center}
    \Large{\textbf{ABSTRACT}}\\
\end{center}
\vspace{1cm}
Automated stock trading faces significant challenges due to market complexity, volatility, and the difficulty of incorporating qualitative information like market sentiment into quantitative models. Traditional trading strategies, such as technical and fundamental analysis, often struggle with sudden market shifts driven by news or sentiment, while pure quantitative models may overlook valuable textual information. Reinforcement Learning has emerged as a promising approach for optimizing trading decisions, yet it typically relies on numerical data and can be data-intensive. Conversely, Large Language Models excel at understanding textual nuances and extracting sentiment from financial news, but they do not inherently make trading decisions, and their integration into robust trading frameworks is an active research area. Existing solutions often treat these components in isolation, limiting their collective potential. This thesis addresses these limitations by proposing an LLM-Augmented Reinforcement Learning framework for automatic stock trading. This approach was chosen to synergistically combine the sequential decision-making strengths of RL with the deep contextual understanding and sentiment extraction capabilities of LLMs. The rationale is that augmenting an RL agent with LLM-derived sentiment signals can provide a richer, more informed market representation, enabling the agent to make more adaptive and potentially more profitable trading decisions, especially in response to news-driven market dynamics that traditional models might miss. The proposed methods outperform both vanilla Reinforcement Learning agents and traditional trading methods.
\begin{flushright}
\begin{tabular}{@{}c@{}}
Student\\
\textit{(Signature and full name)}
\end{tabular}
\end{flushright}
