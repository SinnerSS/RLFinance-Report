Building on chapter \ref{chap:introduction}’s definition of an autonomous trading system that integrates market data and sentiment signals to navigate uncertainty and complexity, chapter 2 reviews prior research and foundational concepts supporting our approach. Section \ref{sec:scope} define the scope and boundaries of our \gls{LLM}-augmented reinforcement learning framework.  Next, section \ref{sec:literature} survey two bodies of academic work: deep reinforcement learning for financial trading and large language model–based sentiment analysis.  We analyze the strengths and limitations of each and highlight gaps our thesis addresses.  Finally, we introduce the core technical foundations—the \gls{SAC} algorithm and methods for extracting financial sentiment with \gls{LLM}s—laying the groundwork for our integrated system.

\section{Scope of Research}
\label{sec:scope}
Our goal is to develop an autonomous stock trading agent that combines advanced \gls{RL} techniques with language-based sentiment signals.  We frame the trading problem as a \gls{MDP} in which the agent observes features of the market and decides on actions (portfolio allocations) to maximize long-term return. The state include historical prices, technical indicators, and crucially, sentiment features derived from textual data (news).  We focus on model-free deep \gls{RL} methods in a continuous-action setting, specifically using the entropy-regularized \gls{SAC} algorithm, due to its stability and exploration benefits.  The environment is assumed to be partially observable and non-stationary, reflecting real market complexity. Key constraints include limited and noisy training data, unpredictable regime shifts, and the need for generalization rather than overfitting.  We do not attempt to model execution details like order matching, nor do we consider alternatives like supervised learning on static data.  Instead, our scope is on the decision-making core: using an RL agent to act continuously in the market, augmented by qualitative signals.

This research deliberately bridges two domains that have often been studied separately.  Traditional quantitative trading models frequently ignore unstructured information, while natural-language methods rarely drive actual trading decisions.  Our focus is on how \gls{LLM}-derived sentiment can be incorporated into an \gls{RL} agent’s state or reward to guide trading.  We assume that sentiment affects market prices (even with delay) and that the \gls{RL} agent can learn to exploit this. Limitations of our study include reliance on historical backtests (no live trading), restricted asset universes, and a single-agent perspective (not modeling other traders). By clarifying these boundaries—an \gls{MDP} framework with added sentiment signals, using off-policy deep \gls{RL} in simulated market data—we set the stage for developing and evaluating \gls{LLM}-augmented RL strategies.

\section{Related Works}
\label{sec:literature}
\subsection{Reinforcement Learning in Financial Trading}
Reinforcement learning has attracted considerable interest for quantitative trading, portfolio management, and execution problems. Surveys note that \gls{RL} methods – especially model-free, policy-based and actor-critic methods – have been applied to tasks like order execution, market-making, hedging, and portfolio optimization \cite{Hambly2023}. These works typically frame finance problems as \gls{MDP}s and leverage deep neural networks to approximate value functions and policies \cite{Kabbani2022, Xia2023, Kabbani2022}. Dang explore value-based \gls{RL} algorithm like \gls{DQN} and its variants Double \gls{DQN} and Dueling \gls{DQN} for trading of US-based stocks \cite{Dang2020}. Xia et al. implemented a trading agent using the \gls{PPO} algorithm and custom reward structure that incorporate risk aversion \cite{Xia2023}. Kodurupaka et al. performed a comprehensive analysis comparing \gls{DQN} and \gls{DDPG} using historical data from S\&P 500 \cite{Kodurupaka2024}. However, each has trade-offs. For example, on-policy methods like \gls{PPO} are robust but sample-inefficient and require fresh data for each update, which is costly in finance \cite{Haarnoja2018}. Off-policy methods like \gls{DDPG} can reuse data but have suffered historically from stability issues \cite{Haarnoja2018}. \gls{SAC} has recently emerged as a state-of-the-art deep \gls{RL} algorithm that mitigates these issues via entropy-regularization.

Empirical studies have shown that \gls{RL}-based trading agents can learn complex, non-linear strategies that often outperform simple benchmarks. For example, Jie et al. constructs a \gls{PPO}-based agent augmented with an \gls{LSTM} to capture temporal trends and cost dynamics \cite{Jie2024}. Other recent work applies deep \gls{RL} to crypto markets, leverage, hedging, and other specialized tasks \cite{Qin2024, LiuKang2024}. These studies highlight \gls{RL}’s advantage of optimizing dynamic strategies with minimal model assumptions. In contrast, classical portfolio models rely on fixed formulas (e.g. Markowitz optimization) that may oversimplify market behavior \cite{Hambly2023}.

Nevertheless, there are well-known challenges. Financial data is noisy, non-stationary, and often limited, which exacerbates \gls{RL}’s sample-complexity and generalization issues. Practitioners warn of severe overfitting: extraordinarily high reported \gls{SR} ratios in some studies likely indicate look-ahead bias or data-snooping \cite{Nikolaos2025}. For instance, \gls{SR} above 3 are statistically rare, yet some \gls{RL}] papers claim \gls{SR}s of 20+, raising skepticism about robustness \cite{Nikolaos2025}. Moreover, formulating the trading problem as an \gls{MDP} itself is difficult: one must select state variables and rewards carefully, and important factors like news or macro shocks are hard to include without exploding dimensionality. Explainability and risk management are also concerns: many deep-\gls{RL} policies are black-box, and naive maximization of reward can lead to “over-greedy” actions that incur tail risks. Overall, while RL offers a powerful framework for sequential decision-making in trading, its practical limitations (data demands, stability, interpretability) motivate hybrid approaches.

\subsection{LLMs and Sentiment Analysis in Financial Forecasting}
Independent of \gls{RL}, a large body of work has explored textual sentiment as a predictor of market movements. Traditional approaches used human-crafted sentiment indices or dictionaries  to score news or reports \cite{Loughran2011}. More recently, neural models have taken over. Transformer-based models such as \gls{BERT} have been fine-tuned on financial text (e.g. FinBERT) to classify sentiment \cite{Kirtac2024, Fatouros2023}. These bidirectional models are optimized for sentiment classification tasks and often achieve good accuracy on labeled datasets.

However, the advent of powerful generative \gls{LLM}s (GPT, LLaMA, etc.) is reshaping the field. Large models pretrained on broad corpora offer deeper contextual understanding and can be used in flexible ways. For example, Fatouros et al. compare ChatGPT (GPT-3.5) to FinBERT on forex news: ChatGPT achieved ~35\% higher classification accuracy and 36\% stronger correlation with actual returns, using only zero-shot prompting \cite{Fatouros2023}. Similarly, Kirtac and Germano (2024) find that an OPT (GPT-3) model achieves 74.4\% sentiment accuracy on US news – far above the 50.1\% accuracy of the Loughran–McDonald dictionary \cite{Kirtac2024}. In portfolio tests, strategies based on OPT sentiment also achieved much higher Sharpe ratios (3.05) than dictionary-based strategies (1.23). These studies highlight that modern LLMs can capture nuanced tone in financial text that older methods miss.

\gls{LLM}s can be employed in various ways: zero-shot or few-shot prompting, fine-tuning on labeled financial sentiment data, or domain-specific pre-training. Prompt engineering has become an art: for zero-shot classification, one might simply instruct “Label the sentiment of this news headline: ...”. For more complex analysis, Chen et al. implemented a Domain Knowledge Chain-of-Thought strategy to intergrate domain specific knowledge with chain-of-thought reasoning \cite{Chen2025}. In parallel, researchers have built finance-specific \gls{LLM}s or adapters (e.g. BloombergGPT, FinGPT, FinLlama) by fine-tuning on financial corpora. Such models combine both the deep linguistic knowledge of \gls{LLM}s and domain-specific vocabulary \cite{Nie2024}.

The advantages of \gls{LLM}-based sentiment analysis are clear: these models capture context and subtleties that lexicons or shallow models cannot \cite{LiuArulappan2024, Kirtac2024}. \gls{GPT} models excel at real-time interpretation of complex news, while \gls{BERT} models provide structured classification ability \cite{LiuArulappan2024}. Empirical evidence shows \gls{LLM}-derived sentiment signals often correlate strongly with future returns \cite{Fatouros2023, Kirtac2024}. Moreover, \gls{LLM}s can handle very long or unstructured texts via summarization. For instance, Chiu and Hung use a LLaMA-2-based summarizer on 10-K filings (MD\&A sections); their \gls{LLM}-derived sentiment signals produced trading strategies with significantly higher buy-and-hold returns than FinBERT-based or traditional methods \cite{Chiu2024}. This suggests that generative AI can “distill” lengthy documents into actionable sentiment.

Nonetheless, \gls{LLM}-based methods also have drawbacks. \gls{LLM}s are computationally intensive and sometimes unpredictable. Without careful prompting or fine-tuning, they may hallucinate or misinterpret financial jargon. Prompted classification can be sensitive to wording and may require domain knowledge (e.g. adding financial context in prompts) to work well \cite{Chen2025}. Domain-specific models like BloombergGPT are proprietary, and open LLMs may still lag in finance-specific accuracy. There is also debate about interpretability: one recent survey notes that \gls{LLM} improve contextual depth but can still be opaque, unlike simple lexicons \cite{Kirtac2024}. In practice, most LLM-sentiment studies remain separate from decision-making algorithms. Few prior works have fully integrated news-based LLM sentiment into an RL decision agent.

This gap motivates our thesis: \gls{LLM}-augmented \gls{RL}. While \gls{RL} can optimize sequential trading strategies, it generally lacks qualitative inputs \cite{Hambly2023}. Conversely, \gls{LLM}s provide powerful sentiment signals but do not themselves make trading decisions. Recent work by Unnikrishnan explicitly combines these: she shows that \gls{RL} models augmented with \gls{LLM}-derived news sentiment earn higher profit and net worth than vanilla \gls{RL} or buy-and-hold strategies \cite{Unnikrishnan2024}. Our work follows this emerging direction. We will leverage the stability and expressiveness of \gls{SAC} for trading, and use \gls{RL} sentiment as part of the state or reward, aiming to capture both quantitative price dynamics and qualitative market mood.

\section{Markov Decision Process}
An \gls{MDP} is formally characterized by a tuple \((S, A, P, R, \gamma)\). The first component, \(S\), represents the State Space, encompassing every possible configuration or situation the environment can be in. A state \(s \in S\), in the ideal \gls{MDP} setting, is assumed to capture all relevant information about the environment necessary for making an informed decision at a given time. The nature of these states can be vastly different, from discrete representations like the positions of pieces on a Go board, to high-dimensional, continuous vectors such as the joint angles and velocities of a robotic arm. A critical assumption underpinning the utility of a state within the pure \gls{MDP} framework is that it provides a sufficient statistic of the history. This means that the past sequence of events leading to the current state does not provide additional predictive power beyond what is already encapsulated in the state itself; the future is conditionally independent of the past, given the present state.

Complementing the state space is the Action Space \(A\), which defines the set of all possible actions \(a \in A\) that the agent can execute. Actions are the means by which the agent influences the environment, and like states, they can be discrete or continuous. The definition of the action space fundamentally constrains the agent's capabilities and can significantly impact the complexity of the learning problem.

The dynamics of the environment, specifying how states evolve in response to actions, are captured by the Transition Probability Function \(P\). This function, \(P(s' | s, a)\), dictates the probability of the environment transitioning to a new state \(s'\) given that the agent is in state \(s\) and takes action \(a\). This probabilistic nature accounts for inherent stochasticity in the environment's response. Crucially, this transition function inherently relies on the Markov Property: the next state \(s'\) depends only on the current state \(s\) and the current action \(a\). Mathematically, this Markov property is expressed as: 
\[P(s_{t+1}| s_t, a_t, s_{t-1}, a_{t-1}, ..., s_0, a_0) = P(s_{t+1} | s_t, a_t)\]

The guiding signal for the agent's learning process is provided by the Reward Function \(R\) This function, often denoted as \(R(s, a, s')\), specifies the scalar immediate reward (or penalty) the agent receives after taking action \(a\) in state \(s\) and transitioning to state \(s'\). The agent's overarching goal is to learn a sequence of actions that maximizes the cumulative sum of these rewards over time. This cumulative reward, known as the return, is often defined as:
\[G_t = \Sigma_{k=0}^\infty r_{t+k+1}\]
where \(\gamma \in [0,1]\) is the discount factor to modulates the importance of future rewards relative to immediate ones.

Within this MDP framework, the agent's decision-making mechanism is formalized as a policy \(\pi\):
\[pi(a|s) = Pr(a_t = a | s_t = s)\] 
which maps states to actions (or distributions over actions). The ultimate aim is to discover an optimal policy \(\pi*\) that maximizes the expected cumulative discounted reward \(E[G_t | s_t = s\) for all state \(s\).

However, the assumption of perfect and complete state observability, central to the \gls{MDP} formulation, is often a strong idealization that does not hold in many complex, real-world scenarios. More frequently, an agent does not have access to the true underlying state of the environment. Instead, it receives observations that are correlated with the true state but may be noisy, incomplete, or ambiguous. This realistic scenario is formally modeled by a \gls{POMDP}.

A \gls{POMDP} extends the \gls{MDP} framework by explicitly acknowledging this imperfect perception. It is typically defined by a tuple \((S, A, P, R, \Omega, O, \gamma)\), where \(S\), \(A\), \(P\), \(R\), and \(\gamma\) are similar to their \gls{MDP} counterparts. The new components are the observation space \(\Omega\), a set of all possible observations \(o \in \Omega\) that the agent can receive from the environment, and the Observation Probability Function \(O(o | s', a)\), which defines the probability of the agent receiving observation \(o\) given that the environment has transitioned to the true state \(s'\) after the agent took action \(a\) in the previous (hidden) state.

In a \gls{POMDP}, since the true state \(s\) is not directly known, the agent cannot base its decisions directly on \(s\). Instead, the agent must operate based on the history of its actions and the observations it has received. To make optimal decisions, an agent in a POMDP often needs to maintain a belief state: 
\[b(s_t) = Pr(s_t | o_t, a_{t-1}, o_{t-1}, ..., a_0, o_0)\] 
which is a probability distribution over the set of all possible true states \(S\), representing the agent's belief about the current true state \(s_t\) given the entire history of past actions and observations up to time \(t\). This belief state can be updated recursively using Bayes' rule: 
\[b_t(s') \propto  O(o_t | s', a_{t-1}) \Sigma_{s \in S} P(s' | s, a_{t-1}) b_{t-1}(s)\] 
The policy in a \gls{POMDP} thus becomes a mapping from belief states (or, more practically, from histories of observations) to actions: \(\pi(a | b)\) or \(\pi(a | h_t)\) where \(h_t = (o_0, a_0, ..., a_{t-1}, o_t)\) is the observation-action history. Finding an optimal policy in a \gls{POMDP} is significantly more challenging than in an \gls{MDP} because the agent must contend with uncertainty about the true state of the world and potentially infer it from a stream of partial information.

The problem of stock trading, which is the focus of this thesis, serves as a quintessential example of a \gls{POMDP}. An \gls{RL} agent attempting to trade stocks does not have access to the complete, true state of the financial market. Factors such as the aggregate sentiment of all market participants, the undisclosed intentions and order books of other traders, unreleased material news, and the true "fair value" of assets remain largely hidden. The agent instead receives observations, such as historical price and volume data, technical indicators, and, in the context of this work, signals derived from \gls{LLM} analyzing news articles, financial reports, and social media. These \gls{LLM}-generated signals, while potentially rich in information, are themselves processed interpretations of complex phenomena and serve as sophisticated observations rather than direct windows into the true, unobservable market state. For instance, an \gls{LLM}-derived sentiment score is an estimate, and different news items or analytical perspectives might lead to conflicting or noisy signals.
