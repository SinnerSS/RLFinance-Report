Building on chapter \ref{chap:introduction}’s definition of an autonomous trading system that integrates market data and sentiment signals to navigate uncertainty and complexity, chapter 2 reviews prior research and foundational concepts supporting our approach. Section \ref{sec:scope} define the scope and boundaries of our \gls{LLM}-augmented reinforcement learning framework.  Next, section \ref{sec:literature} survey two bodies of academic work: deep reinforcement learning for financial trading and large language model–based sentiment analysis.  We analyze the strengths and limitations of each and highlight gaps our thesis addresses.  Finally, we introduce the core technical foundations—the \gls{SAC} algorithm and methods for extracting financial sentiment with \gls{LLM}s—laying the groundwork for our integrated system.

\section{Scope of Research}
\label{sec:scope}
Our goal is to develop an autonomous stock trading agent that combines advanced \gls{RL} techniques with language-based sentiment signals.  We frame the trading problem as a \gls{MDP} in which the agent observes features of the market and decides on actions (portfolio allocations) to maximize long-term return. The state include historical prices, technical indicators, and crucially, sentiment features derived from textual data (news).  We focus on model-free deep \gls{RL} methods in a continuous-action setting, specifically using the entropy-regularized \gls{SAC} algorithm, due to its stability and exploration benefits.  The environment is assumed to be partially observable and non-stationary, reflecting real market complexity. Key constraints include limited and noisy training data, unpredictable regime shifts, and the need for generalization rather than overfitting.  We do not attempt to model execution details like order matching, nor do we consider alternatives like supervised learning on static data.  Instead, our scope is on the decision-making core: using an RL agent to act continuously in the market, augmented by qualitative signals.

This research deliberately bridges two domains that have often been studied separately.  Traditional quantitative trading models frequently ignore unstructured information, while natural-language methods rarely drive actual trading decisions.  Our focus is on how \gls{LLM}-derived sentiment can be incorporated into an \gls{RL} agent’s state or reward to guide trading.  We assume that sentiment affects market prices (even with delay) and that the \gls{RL} agent can learn to exploit this. Limitations of our study include reliance on historical backtests (no live trading), restricted asset universes, and a single-agent perspective (not modeling other traders). By clarifying these boundaries—an \gls{MDP} framework with added sentiment signals, using off-policy deep \gls{RL} in simulated market data—we set the stage for developing and evaluating \gls{LLM}-augmented RL strategies.

\section{Related Works}
\label{sec:literature}
\subsection{Reinforcement Learning in Financial Trading}
Reinforcement learning has attracted considerable interest for quantitative trading, portfolio management, and execution problems. Surveys note that \gls{RL} methods – especially model-free, policy-based and actor-critic methods – have been applied to tasks like order execution, market-making, hedging, and portfolio optimization \cite{Hambly2023}. These works typically frame finance problems as \gls{MDP}s and leverage deep neural networks to approximate value functions and policies \cite{Kabbani2022, Xia2023, Kabbani2022}. Dang explore value-based \gls{RL} algorithm like \gls{DQN} and its variants Double \gls{DQN} and Dueling \gls{DQN} for trading of US-based stocks \cite{Dang2020}. Xia et al. implemented a trading agent using the \gls{PPO} algorithm and custom reward structure that incorporate risk aversion \cite{Xia2023}. Kodurupaka et al. performed a comprehensive analysis comparing \gls{DQN} and \gls{DDPG} using historical data from S\&P 500 \cite{Kodurupaka2024}. However, each has trade-offs. For example, on-policy methods like \gls{PPO} are robust but sample-inefficient and require fresh data for each update, which is costly in finance \cite{Tuomas2018}. Off-policy methods like \gls{DDPG} can reuse data but have suffered historically from stability issues \cite{Tuomas2018}. \gls{SAC} has recently emerged as a state-of-the-art deep \gls{RL} algorithm that mitigates these issues via entropy-regularization.

Empirical studies have shown that \gls{RL}-based trading agents can learn complex, non-linear strategies that often outperform simple benchmarks. For example, Jie et al. constructs a \gls{PPO}-based agent augmented with an \gls{LSTM} to capture temporal trends and cost dynamics \cite{Jie2024}. Other recent work applies deep \gls{RL} to crypto markets, leverage, hedging, and other specialized tasks \cite{Qin2024, LiuKang2024}. These studies highlight \gls{RL}’s advantage of optimizing dynamic strategies with minimal model assumptions. In contrast, classical portfolio models rely on fixed formulas (e.g. Markowitz optimization) that may oversimplify market behavior \cite{Hambly2023}.

Nevertheless, there are well-known challenges. Financial data is noisy, non-stationary, and often limited, which exacerbates \gls{RL}’s sample-complexity and generalization issues. Practitioners warn of severe overfitting: extraordinarily high reported \gls{SR} ratios in some studies likely indicate look-ahead bias or data-snooping \cite{Nikolaos2025}. For instance, \gls{SR} above 3 are statistically rare, yet some \gls{RL}] papers claim \gls{SR}s of 20+, raising skepticism about robustness \cite{Nikolaos2025}. Moreover, formulating the trading problem as an \gls{MDP} itself is difficult: one must select state variables and rewards carefully, and important factors like news or macro shocks are hard to include without exploding dimensionality. Explainability and risk management are also concerns: many deep-\gls{RL} policies are black-box, and naive maximization of reward can lead to “over-greedy” actions that incur tail risks. Overall, while RL offers a powerful framework for sequential decision-making in trading, its practical limitations (data demands, stability, interpretability) motivate hybrid approaches.

\subsection{LLMs and Sentiment Analysis in Financial Forecasting}
Independent of \gls{RL}, a large body of work has explored textual sentiment as a predictor of market movements. Traditional approaches used human-crafted sentiment indices or dictionaries  to score news or reports \cite{Loughran2011}. More recently, neural models have taken over. Transformer-based models such as \gls{BERT} have been fine-tuned on financial text (e.g. FinBERT) to classify sentiment \cite{Kirtac2024, Fatouros2023}. These bidirectional models are optimized for sentiment classification tasks and often achieve good accuracy on labeled datasets.

However, the advent of powerful generative \gls{LLM}s (GPT, LLaMA, etc.) is reshaping the field. Large models pretrained on broad corpora offer deeper contextual understanding and can be used in flexible ways. For example, Fatouros et al. compare ChatGPT (GPT-3.5) to FinBERT on forex news: ChatGPT achieved ~35\% higher classification accuracy and 36\% stronger correlation with actual returns, using only zero-shot prompting \cite{Fatouros2023}. Similarly, Kirtac and Germano (2024) find that an OPT (GPT-3) model achieves 74.4\% sentiment accuracy on US news – far above the 50.1\% accuracy of the Loughran–McDonald dictionary \cite{Kirtac2024}. In portfolio tests, strategies based on OPT sentiment also achieved much higher Sharpe ratios (3.05) than dictionary-based strategies (1.23). These studies highlight that modern LLMs can capture nuanced tone in financial text that older methods miss.

\gls{LLM}s can be employed in various ways: zero-shot or few-shot prompting, fine-tuning on labeled financial sentiment data, or domain-specific pre-training. Prompt engineering has become an art: for zero-shot classification, one might simply instruct “Label the sentiment of this news headline: ...”. For more complex analysis, Chen et al. implemented a Domain Knowledge Chain-of-Thought strategy to intergrate domain specific knowledge with chain-of-thought reasoning \cite{Chen2025}. In parallel, researchers have built finance-specific \gls{LLM}s or adapters (e.g. BloombergGPT, FinGPT, FinLlama) by fine-tuning on financial corpora. Such models combine both the deep linguistic knowledge of \gls{LLM}s and domain-specific vocabulary \cite{Nie2024}.

The advantages of \gls{LLM}-based sentiment analysis are clear: these models capture context and subtleties that lexicons or shallow models cannot \cite{LiuArulappan2024, Kirtac2024}. \gls{GPT} models excel at real-time interpretation of complex news, while \gls{BERT} models provide structured classification ability \cite{LiuArulappan2024}. Empirical evidence shows \gls{LLM}-derived sentiment signals often correlate strongly with future returns \cite{Fatouros2023, Kirtac2024}. Moreover, \gls{LLM}s can handle very long or unstructured texts via summarization. For instance, Chiu and Hung use a LLaMA-2-based summarizer on 10-K filings (MD\&A sections); their \gls{LLM}-derived sentiment signals produced trading strategies with significantly higher buy-and-hold returns than FinBERT-based or traditional methods \cite{Chiu2024}. This suggests that generative AI can “distill” lengthy documents into actionable sentiment.

Nonetheless, \gls{LLM}-based methods also have drawbacks. \gls{LLM}s are computationally intensive and sometimes unpredictable. Without careful prompting or fine-tuning, they may hallucinate or misinterpret financial jargon. Prompted classification can be sensitive to wording and may require domain knowledge (e.g. adding financial context in prompts) to work well \cite{Chen2025}. Domain-specific models like BloombergGPT are proprietary, and open LLMs may still lag in finance-specific accuracy. There is also debate about interpretability: one recent survey notes that \gls{LLM} improve contextual depth but can still be opaque, unlike simple lexicons \cite{Kirtac2024}. In practice, most LLM-sentiment studies remain separate from decision-making algorithms. Few prior works have fully integrated news-based LLM sentiment into an RL decision agent.

This gap motivates our thesis: \gls{LLM}-augmented \gls{RL}. While \gls{RL} can optimize sequential trading strategies, it generally lacks qualitative inputs \cite{Hambly2023}. Conversely, \gls{LLM}s provide powerful sentiment signals but do not themselves make trading decisions. Recent work by Unnikrishnan explicitly combines these: she shows that \gls{RL} models augmented with \gls{LLM}-derived news sentiment earn higher profit and net worth than vanilla \gls{RL} or buy-and-hold strategies \cite{Unnikrishnan2024}. Our work follows this emerging direction. We will leverage the stability and expressiveness of \gls{SAC} for trading, and use \gls{RL} sentiment as part of the state or reward, aiming to capture both quantitative price dynamics and qualitative market mood.
